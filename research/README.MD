# PromptKG

**Large-scale Knowledge Graph Construction and Application with Prompting across ```tasks``` (predictive and generative), and ```modalities``` (language, image, vision + language, etc.)**

<!--## Large-scale Knowledge Graph Construction with Prompting Across Tasks and Modalities-->

> [**GenKGC**](https://github.com/zjunlp/PromptKG/tree/main/research/GenKGC):  link prediction as sequence-to-sequence generation for fast inference

> [**KnowCo-Tuning**](https://github.com/zjunlp/PromptKG/tree/main/research/KnowCo-Tuning): data-efficient prompt learning-based knowledge graph completion


## News
- [Model Release] Jaunary, 2022: [**GenKGC**](https://github.com/zjunlp/PromptKG/tree/main/research/GenKGC) - A sequence-to-sequence approach for knowledge graph completion.
- [Model Release] January, 2022: [**KnowCo-Tuning**](https://github.com/zjunlp/PromptKG/tree/main/research/KnowCo-Tuning) - A prompt learning-based approach for few-shot knowledge graph completion


## Release

**\*\*\*\*\* ```January, 2022```: [GenKGC](https://github.com/zjunlp/PromptKG/tree/main/research/GenKGC) | [KnowCo-Tuning](https://github.com/zjunlp/PromptKG/tree/main/research/KnowCo-Tuning) release \*\*\*\*\***

- [x] [**GenKGC**](https://github.com/zjunlp/PromptKG/tree/main/research/GenKGC) (Jaunary 31, 2020): GenKGC converts knowledge graph completion to sequence-to-sequence generation with pre-trained language model with relation-guided demonstration and entity-aware hierarchical decoding. It can obtain better or comparable performance than baselines, and achieve faster inference speed compared with previous methods with pre-trained language models. "[From Discrimination to Generation: Knowledge Graph Completion with Generative Transformer](https://arxiv.org/pdf/2202.02113.pdf)```WWW 2022``` "
- [x] [**KnowCo-Tuning**](https://github.com/zjunlp/PromptKG/tree/main/research/KnowCo-Tuning) (Jaunary 31, 2020): A prompt-tuning approach (knowledge collaborative fine-tuning) for low-resource knowledge graph completion. KG-Prompt leverages the structured knowledge to construct the initial prompt template and learn the optimal templates, labels and model parameters through a collaborative fine-tuning algorithm.  It can obtain state-of-the-art few-shot performance on FB15K-237, WN18RR, and UMLS. "[Knowledge Collaborative Fine-tuning for Low-resource Knowledge Graph Completion (基于知识协同微调的低资源知识图谱补全方法)](http://jos.org.cn/jos/article/abstract/6628?st=search) ```Journal of Software 2022 (软件学报)```"




### Contact Information

For help or issues using the models, please submit a GitHub issue.

For other communications, please contact [Ningyu Zhang](https://person.zju.edu.cn/en/ningyu/).
